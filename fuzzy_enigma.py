# -*- coding: utf-8 -*-
"""fuzzy_enigma

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J5nFB29tDIkkh4Za83UYAZzweJuE6be2

# Price Estimation for Berlin Airbnb Data: Machine Learning & Deep Learning Class

## Below you can find the full list of variables present in the data set

- id - ID of an offer
- listing_url - Link to the offer on Airbnb
- scrape_id - Scrapped id timestamp
- last_scrapped - Time of the last webscraping of the data
- name - Name of the offer
- summary - Quick overview of the property (text)
- space - Quick overview of the space features (text)
- description - Full description of the offer (text)
- experiences_offered - Additional experiences available in the offer (text)
- neighborhood_overview - Description of the neighborhood (text)
- notes - Message to customers (text)
- transit - Information about public transport in the neighborhood of the property (text)
- access - Way to access the property (text)
- interaction - Possibilities to interact with landlord (text)
- house_rules - Rules of the property (text)
- thumbnail_url - EMPTY
- medium_url - EMPTY
- picture_url - Link to the featured picture
- xl_picture_url - EMPTY
- host_id - ID of the landlord
- host_url - Link to the landlord Airbnb homepage
- host_name - Name of the landlord
- host_since - Date when landlord created the account
- host_location - Landlord place of living (text)
- host_about - Short description of the landlord (text)
- host_response_time - Average response time of the landlord (text)
- host_resposne_rate - Amount of message responded by the landlord
- host_acceptance_rate - The rate of guest accepted
- host_is_superhost - Is landlord a superhost? True/False (binary)
- host_thumbnail_url - Thumbnail picture of the host
- host_picture_url - Full picture of the host
- host_neighbourhood - Neighborhood of the landlord place (text)
- host_listings_count - Number of properties offered for rent by landlord
- host_total_listings_count - Number of properties offered for rent by host at total
- host verifications - Types of verifications provided by landlord (dict)
- host_has_profile_pic - Does landlord has a profile picture? True/False (binary)
- host_identity_verified - Does landlord verified his/her identity? True/False (binary)
- street - Street where is the property (text)
- neighbourhood - Name of the neighborhood of the property (text)
- neigbourhood_cleansed - Full name of the neighborhood (text)
- neighbourhood_group_cleansed - Neighborhood group name (text)
- city - City where the property is offered (text)
- state - State where the property is offered (text)
- zipcode - Zipcode of the property
- market - ?
- smart_location - Smart location search by Google (text)
- country_code - Country code (text)
- country - Country (text)
- latitude - Latitude
- longitude - Longtitude
- is_location_exact - Is location exact? True/False (binary)
- property_type - Type of the property (text)
- room_type - Type of the offered commodity (single room, entire place etc.) (text)
- accomodates - Accomodates
- bathrooms - Number of bathrooms
- beds - Number of beds
- bed_type - Type of bed offered (single,double,queen etc.) (text)
- amenities - Additional amenities offered (wifi,elevator etc.) (text)
- square_feet - Square feet
- price - Average price of the offer 
- weekly_price - Average weekly price of the offer (if host is offering some discount for weekly+ stays)
- monthly_price - Average monthly price of the offer (if host is offering some discount for monthly+ stays)
- security_deposit - Security deposit required to rent
- cleaning_fee - Cleaning fee
- guests_included - How many ppl can stay at the property (ex. Suitable for 2 ppl)
- extra_people - Fee for additional number of ppl staying in the property
- minimum_nights - Minimum number of nights to rent the property
- maximum_nights - Maximum number of nights to rent the property
- calendar_updated - How long time ago the landlord updated the property rent callendar
- has_availability - Is it available? True/False (binary)
- availability_30 - ?
- availability_60 - ?
- availability_90 - ?
- availability_365 - ?
- calendar_last_scraped - Date of the last scrapping the property calendar
- number_of_reviews - Number of reviews 
- first_review - When the first review was written
- last_review - When the last review was written
- review_scores_rating - Overall rating of the property
- review_scores_accuracy - Rating of the accuracy of the description to match offer
- review_scores_cleanlines - Rating of the cleanlines of the offer
- review_scores_checkin - Rating of the checking hospitality and time
- review_scores_communication - Rating of the communication with landlord
- review_scores_location - Rating of the neighboorhood and location of the property
- review_scores_value - Overall value of the property
- require_license - Have provided lincense?
- license - number of the license
- jurisdiction_names - ?
- instant_bookable - Is instant bookable? True/False (binary)
- is_business_travel_ready - Is business travel ready? True/False (binary)
- cancellation_policy - Cancellation policy terms (text)
- require_guest_profile_picture - Does landlord require guest profile picture? True/False (binary)
- require_guest_phone_verified - Does landlord require guest phone number been verified? True/False (binary)
- calculated_host_listings - How many listings landlord offered?
- reviews_per_month - Amount of reviews per month for the property

![](https://travelhero.pl/wp-content/uploads/2018/09/airbnb.png)

## Airbnb: An Overview
Airbnb is an online marketplace that connects people who want to rent out their homes with people who are looking for accommodations in that locale. It currently covers more than 81,000 cities and 191 countries worldwide. The company's name comes from "air mattress B&B.‚Äù

For hosts, participating in Airbnb is a way to earn some income from their property, but with the risk that the guest might do damage to it. For guests, the advantage can be relatively inexpensive accommodations, but with the risk that the property won't be as appealing as the listing made it seem.

# Libraries
"""

# Standard stuff
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from random import shuffle
import gc
import re
from random import shuffle

# Statsmodel 
import statsmodels.api as sm
import statsmodels.formula.api as smf

# XGBoost
import xgboost as xgb
from scipy import stats # F1 score

# Random forest
import sklearn as sk
from sklearn import metrics, neighbors, linear_model
from sklearn.model_selection import RandomizedSearchCV, KFold, cross_val_predict, train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import mean_squared_error,  r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler, LabelEncoder


# Tensorflow and Keras
import tensorflow as tf
import tensorflow.python.keras
from tensorflow.python.keras.layers import Dense, Dropout
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.callbacks import History, EarlyStopping
from tensorflow.python.keras.optimizers import SGD

from geopy.distance import great_circle

#warnings.filterwarnings("ignore")

# Center all plots by CSS
from IPython.core.display import HTML

HTML("""
<style>
.output_png {
    display: table-cell;
    text-align: center;
    margin:auto;
    }
.prompt 
    display:none;
}  
</style>
""")

"""# Data Import
Data source: https://www.kaggle.com/brittabettendorf/berlin-airbnb-data
"""

from google.colab import drive
drive.mount('/Fuzzy')

pd.set_option("display.max_columns",30)
pd.set_option('display.float_format', lambda x: '%.2f' %x)
data = pd.read_csv('/Fuzzy/My Drive/Fuzzy Enigma/data/listings_summary.csv')

data.info()

data.head()

data.describe()

"""In our DataSet we have roughly 22 552 observations and 99 variables. Significant amount of them are NA or not needed (like for example photo thumbnails), and at the first glimpse we can see variables that will need to be removed in future due to the fact that their can be strongly correlated with eachother - as far as our domain knowledge from the previous projects tells us.

# Explanatory Data Analysis (EDA)

In this part we will perform some Data Engineering, Visualisations etc. to make our variables more efficient for future purposes

![](https://www.gre.ac.uk/__data/assets/image/0011/1191953/analysis-banner.jpg)

## Feature Engineering

First we will delete create list with the names of columns we want to delete from our data set, either due to the inconsistency, lack of information or they are just not needed.
 - We have divided our useless columns to four categories
     - descriptive columns: which we do not need because we do not want to perform any text analysis, and we belive description of the property doesnt affect it market price. Also some descriptions are in German...
     - empty columns: columns which are mostly in 100% NA
     - inconsistent columns: columns with data that are redundant, or not needed for our analysis i.e pictures thumbnail urls etc.
     - doubtful columns: columns which does not provide any reasonable information, or their origin are doubtful and are not easy to describe what they are

As we decided not introducing here any form of text mining, we will also get rid of all description of properties etc - because we think it is strongly subjective topic which depends mostly on the landlord abilities to write nice text
"""

desc_cols = ["name","summary","space","neighborhood_overview","notes","transit","access","interaction","house_rules","host_about","host_location","host_neighbourhood","neighbourhood"]
empty_cols = ["experiences_offered","host_acceptance_rate","license","jurisdiction_names"]
inconsistent_cols = ["id","listing_url","scrape_id","last_scraped","thumbnail_url","medium_url","picture_url","xl_picture_url","host_id","host_url","host_name","host_since","host_thumbnail_url","host_picture_url","street","city","state","zipcode","market","country_code","country","square_feet"]
doubtful_cols = ["host_listings_count","host_total_listings_count","host_verifications","smart_location","is_location_exact","weekly_price","monthly_price","calendar_updated","has_availability","availability_30","availability_60","availability_90","availability_365","calendar_last_scraped","first_review","last_review","requires_license","calculated_host_listings_count"]

"""We have also to do some cleaning to the data itself, i.e change some NULL's to 0 and conversely:
- security_deposit: change null to 0
- cleaning_fee: change null to 0
- cancellation_policy: shorten text to; flexible, moderate,strict
- host_response_time: shorten text to: hours, day, NA
- host_is_superhost: mark; t->1 f->0
- host_has_profile_pic: mark; t->1 f->0
- host_identity_verified: mark; t->1 f->0
- instant_bookable: mark; t->1 f->0
- is_business_travel_ready: mark; t->1 f->0
- require_guest_profile_picture: mark; t->1 f->0
- require_guest_phone_verification: mark; t->1 f->0
"""

#cleaning amenities
from collections import Counter
results = Counter()
data['amenities'].str.strip('{}')\
               .str.replace('"', '')\
               .str.lstrip('\"')\
               .str.rstrip('\"')\
               .str.split(',')\
               .apply(results.update)

data.amenities = data['amenities'].str.strip('{}')\
               .str.replace('"', '')\
               .str.lstrip('\"')\
               .str.rstrip('\"')\
               .str.split(',')

#cleaning prices
data.price = data['price'].str.replace("$",'')
data.price = data['price'].str.replace(",",'')
data.security_deposit = data['security_deposit'].str.replace("$",'')
data.security_deposit = data['security_deposit'].str.replace(",",'')
data.cleaning_fee = data['cleaning_fee'].str.replace("$",'')
data.cleaning_fee = data['cleaning_fee'].str.replace(",",'')
data.extra_people = data['extra_people'].str.replace("$",'')
data.extra_people = data['extra_people'].str.replace(",",'')

#filling NA's with 0 where relevant
data.security_deposit=data.security_deposit.fillna("0")
data.cleaning_fee = data.cleaning_fee.fillna("0")
data.extra_people = data.cleaning_fee.fillna("0")
data = data.dropna(subset=['host_is_superhost', 'host_has_profile_pic','host_identity_verified'])

#creating list with colnames for converting from t to 1 and f to 0
convert_cols = ["host_is_superhost","host_has_profile_pic","host_identity_verified","instant_bookable","is_business_travel_ready","require_guest_profile_picture","require_guest_phone_verification"]

#looping over rows and checking statement: IF t then -> 1 , ELSE 0
mapa = {'t': True, 'f': False}
for i in convert_cols:
    data[i]= data[i].map(mapa)

#creating only 5 groups for Cancellation Policy
data.cancellation_policy = pd.np.where(data.cancellation_policy.str.contains("flexible"),"flexible",
                        pd.np.where(data.cancellation_policy.str.contains("moderate"),"moderate",
                        pd.np.where(data.cancellation_policy.str.contains("super_strict_60"),"extremaly strict",
                        pd.np.where(data.cancellation_policy.str.contains("super_strict_30"),"very strict",
                       pd.np.where(data.cancellation_policy.str.contains("strict_14"),"strict","NA")))))

data.host_response_time = pd.np.where(data.host_response_time.str.contains("within an hour"),"less than hour",
                        pd.np.where(data.host_response_time.str.contains("within a few hours"),"few hours",
                        pd.np.where(data.host_response_time.str.contains("within a day"),"day",
                        pd.np.where(data.host_response_time.str.contains(""),"NA",
                       pd.np.where(data.host_response_time.str.contains("N/A"),"NA","NA")))))

#lets finally drop unneccessary columns
drop_list = desc_cols + empty_cols + inconsistent_cols + doubtful_cols
data = data.drop(drop_list,axis=1)

#Final step is to make response rate normal number and to convert all objects to floats
data.host_response_rate = data['host_response_rate'].str.replace('%','')\
               .str.replace('NaN', 'NA')
data.host_response_rate = data.host_response_rate.astype(float)
data.host_response_rate = data.host_response_rate.div(100)

#converting to floats
to_float = ['longitude','latitude','price','security_deposit','cleaning_fee','extra_people']

for i in to_float:
    data[i] = data[i].astype(float)

"""As we found it helpful in some kaggle kernel we will use their work to create new colum:
   - size
   - distance
    
These one will help us deal with our missing data in the column square_feet. 
Basically we will make use of extraction numbers from property overview to estimate the size of the property.
"""

data['size'] = data['description'].str.extract('(\d{2,3}\s?[smSM])', expand=True)
data['size'] = data['size'].str.replace("\D", "")

# change datatype of size into float
data['size'] = data['size'].astype(float)
data.drop(['description'], axis=1, inplace=True)

def distance_to_mid(lat, lon):
    berlin_centre = (52.5027, 13.4041)
    accommodation = (lat, lon)
    return great_circle(berlin_centre, accommodation).km

data['distance'] = data.apply(lambda x: distance_to_mid(x.latitude, x.longitude), axis=1)

for i in data.columns:
    print(i,":",data[i].isna().sum()/len(data[i]))

"""We'll fill NAs with mean values. This should have good results for this dataset."""

data[['host_response_rate', 'reviews_per_month', 'size']] = data[['host_response_rate', 'reviews_per_month', 'size']].fillna(data[['host_response_rate', 'reviews_per_month', 'size']].mean())

"""## Plotting

![](https://images.unsplash.com/photo-1434626881859-194d67b2b86f?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1053&q=80)

We plotted as much as possible to see the data in visual form, mostly because we are visual learners and it's easier to interpret for us than just standard table.

We will also try not to plot everything, only those variables that we think are crucial to plot.

### Amenities
"""

sub_df = pd.DataFrame(results.most_common(30), columns=['amenity', 'count'])
sub_df.sort_values(by=['count'], ascending=True).plot(kind='barh', x='amenity', y='count',  
                                                      figsize=(10,7), legend=False, color='darkgrey',
                                                      title='Amenities')
plt.xlabel('Count');

"""These are top 30 amenities that are present in properties offered in Berlin

### Price
"""

plt.figure(figsize=(16,12))
plot = sns.countplot(data['price'].sort_values())
for ind, label in enumerate(plot.get_xticklabels()):
    if ind % 25 == 0:  # every 10th label is kept
        label.set_visible(True)
    else:
        label.set_visible(False)
plt.title("Prices of the properties in Berlin",fontsize=30)
plot.set_xlabel("Price",fontsize=15)
plot.tick_params(labelsize=10)
plt.show()

"""- We can see here that the price range for most properties is around 30 per night, but we can see outliers which shows that some properties price is EVEN 1000 per night

 - Lets drop then the outliers (price>250) for further analysis
"""

data = data.query("price>5 & price < 300")

plt.figure(figsize=(16,12))
plot = sns.countplot(data['price'].sort_values())
for ind, label in enumerate(plot.get_xticklabels()):
    if ind % 25 == 0:  # every 10th label is kept
        label.set_visible(True)
    else:
        label.set_visible(False)
plt.title("Prices of the properties in Berlin",fontsize=30)
plot.set_xlabel("Price",fontsize=15)
plot.tick_params(labelsize=10)
plt.show()

"""Now it looks way better!

### Size of the properties
"""

plt.figure(figsize=(16,12))
plot = sns.countplot(data['size'])
for ind, label in enumerate(plot.get_xticklabels()):
    if ind % 25 == 0:  # every 10th label is kept
        label.set_visible(True)
    else:
        label.set_visible(False)
plt.title("Square meters distribution in properties",fontsize=30)
plot.set_xlabel("Square meters",fontsize=15)
plot.tick_params(labelsize=10)
plt.show()

"""### Description of the Host"""

fig, ax = plt.subplots(2,2,figsize=(30,20))
plot1 = sns.countplot(data['host_response_rate'].sort_values(), ax=ax[0][0])
plot2 = sns.countplot(data['host_response_time'].sort_values(), ax=ax[0][1])
plot3 = sns.countplot(data['host_is_superhost'].sort_values(), ax=ax[1][0])
plot4 = sns.countplot(data['host_identity_verified'].sort_values(), ax=ax[1][1])
for ind, label in enumerate(plot1.get_xticklabels()):
    if ind % 25 == 0:  # every 10th label is kept
        label.set_visible(True)
    else:
        label.set_visible(False)
plot1.set_xlabel("host response rate",fontsize=35)
plot1.tick_params(labelsize=25)
plot2.set_xlabel("host response time",fontsize=35)
plot2.tick_params(labelsize=25)
plot3.set_xlabel("host is superhost",fontsize=35)
plot3.tick_params(labelsize=25)
plot4.set_xlabel("host identity verified",fontsize=35)
plot4.tick_params(labelsize=25)
plt.show()

"""- We can see from here that hosts response rate overall is very high, and majority of them replies within hours, however we need to delete these rows which are 0 for the sake of clarity

- Yet majority of host are not superhost and there are more unverified than verified hosts
"""

data.drop(['host_response_rate'], axis=1, inplace=True)

"""### Price with respect to longitude and latitude"""

data.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4, figsize=(20,15), 
        c="price", cmap="hot", colorbar=True, sharex=False); # The ; lets us skip the object

"""### Lets just take a quick glimpse at all variables"""

fig = plt.figure(figsize=(20, 15))

for i, col in enumerate(data.columns[9:15]):
    ax = fig.add_subplot(3, 2, i + 1)
    if len(data[col].unique()) > 20:
        sns.distplot(data[col])
    else:
        sns.countplot(data[col])

fig.tight_layout(pad=3)

"""- We have to deal somehow with outliers - so as we do in everycase, we will just cut them
    - bedrooms: above 4
    - accomodates: bin everything above 7 as 7
    - bathrooms: above 2
    - beds: bin everything above 5 as 5
"""

data = data.query("accommodates<7")
data = data.query("beds>0 & beds<5")
data = data.query("bathrooms>0 & bathrooms<3")
data = data.query("bedrooms<4")

fig = plt.figure(figsize=(20,15))

for i, col in enumerate(data.columns[18:23]):
    ax = fig.add_subplot(3, 2, i + 1)
    if len(data[col].unique()) > 20:
        sns.distplot(data[col])
    else:
        sns.countplot(data[col])

fig.tight_layout(pad=3)

"""- We can see that variable maximum_night is useless lets remove it
- Additionaly we will cut the outliers in:
    - cleaning_fee: above 60
    - guests_included: above 4
    - extra_people: above 40
    - minimum_nights: above 30
    - number_of_reviews: above 60
"""

data = data.query("cleaning_fee<60")
data = data.query("security_deposit<600")
data = data.query("guests_included<5")
data = data.query("extra_people<40")
data = data.query("number_of_reviews<50")
data.drop(['maximum_nights'], axis=1, inplace=True)
data.drop(['minimum_nights'], axis=1, inplace=True)

fig = plt.figure(figsize=(16,40))

for i, col in enumerate(data.columns[23:34]):
    ax = fig.add_subplot(15, 3, i + 1)
    if len(data[col].unique()) > 20:
        sns.distplot(data[col])
    else:
        sns.countplot(data[col])

fig.tight_layout(pad=3)

"""- We can see that variable is_business_travel_ready is not needed
- We will also cut some outliers from:
    - review_scores_value: below 5 and everything less than 7 as 7
    - review_scores_checkin: below 5 and everything less than 7 as 7
    - review_scores_cleanliness: below 5
    - review_scores_communication: below 7 and everything less than 8 as 8
    - review_scores_location: everything below 7 as 7
    - review_scores_accuracy: everything below 7 as 7
"""

data.drop(['is_business_travel_ready'], axis=1, inplace=True)
data.drop(['require_guest_profile_picture'], axis=1, inplace=True)
data.drop(['require_guest_phone_verification'], axis=1, inplace=True)
data = data.query('review_scores_accuracy>7')
data = data.query("review_scores_cleanliness>5")
data = data.query('review_scores_checkin>7')
data = data.query('review_scores_communication>7')
data = data.query('review_scores_location>6')
data = data.query('review_scores_value>6')
data = data.query("reviews_per_month<4")
data = data.query('distance<15')

plt.figure(figsize=(14,8))
plot = sns.countplot(data['host_identity_verified'])
for ind, label in enumerate(plot.get_xticklabels()):
    if ind % 25 == 0:  # every 10th label is kept
        label.set_visible(True)
    else:
        label.set_visible(False)
plt.title("Does host verified his accound with ID?",fontsize=30)
plot.set_xlabel("0 = False, 1 = True",fontsize=15)
plot.tick_params(labelsize=10)
plt.show()

"""## Investigating Correlations among variables

- Here we will investigate correlation among our variables
- We will also make finall feature selection to minimize our variable set (we have 41 of them!!!)
"""

f,ax = plt.subplots(figsize=(20, 15))
sns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)
plt.show()

"""As we could predict, we have some strongly correlated variables:
  - accomodates - beds (0.7)
  - reviews_score_rating - reviews_score_accuracy (0.6)
  - reviews_score_rating - reviews_score_cleanliness (0.6)
  - reviews_score_rating - reviews_score_value (0.6)
  - is_space - is_neighborhood_overview (0.6)
  - is_space - is_transit (0.7)
  - is_neigborhood_overview - is_transit (0.7)

However we wont delete it because we will estimate kind of models that deal with it well

We will also take a look at our variable basket and delete some corresponding to eachother, or just these ones that we think are too obvious or not entirely necessary

We will get rid of:
   - host_has_profile_pic: we do not think that profile picture affects price anyhow
   - neighbourhood_group_cleansed: because it is more general than neighbourhood_cleansed
   - latitude
   - longitude
   - property_type: because more important info is in room_type
   - amenities: they are too many
   - minimum_nights: not consistent - it is obvious that for most it will be 0/1
   - is_note: is basically the same as is_house_rules
   - review_scores_value: bigger category for other review_scores variables
   - host_identity_verified: weird variable, because all host should be verified to put an offer
"""

data = data.drop(columns = ['neighbourhood_group_cleansed','neighbourhood_cleansed','host_has_profile_pic','latitude','longitude','amenities','property_type'])

"""# Model Estimation

![](https://cdn.lynda.com/course/645050/645050-636700308369503992-16x9.jpg)
Let's look at the data.
"""

data.describe()

"""Overall, we dropped about half the observations due to errors, outliers etc."""

data.info()

"""We'll drop NA just in case, Scikit models won't work with them."""

data = data.dropna()

"""Create a hard copy for futher processing."""

data_copy = data.copy().reset_index(drop=True)

"""At the beginning let's convert all string columns into categorical ones:"""

for col in ['host_response_time', 'room_type', 'bed_type', 'cancellation_policy']:
    data_copy[col] = data_copy[col].astype('category')

"""Next, shuffle the data."""

data_copy = data_copy.sample(frac=1).reset_index(drop=True)

"""Then, scale the numerical values and create dummies."""

# Extract the 
Y =  data_copy['price']
data_copy.drop(['price'], axis=1, inplace=True)

# Select numerical var
numerical_features = data_copy.select_dtypes(include=['float64', 'int64', 'bool']).copy()

# Save the column names
cols = numerical_features.columns

# Scale the numerical vars
scaler = StandardScaler()
numerical_features = scaler.fit_transform(numerical_features)
numerical_features = pd.DataFrame(numerical_features, columns = cols)

# Select categorical var
categorical_features = data_copy.select_dtypes(include=['category']).copy()

# Create dummies for categorical 
categorical_features = pd.get_dummies(categorical_features)

# Glue them back 
data_copy = pd.concat([numerical_features, categorical_features], axis=1)

X = data_copy

"""All numerical variables are standarised, as expected. Categorical variables are hot-encoded."""

X.describe()

"""## Splitting the data
We'll use a 70% train, 30% test split for our model estimation.
"""

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3)

"""Let us create a list to hold the final results."""

results = []

"""# Models
## Random Forest
We'll estimate 1000 trees with a maximal depth of 7 levels. Since the dataset is quite small, the estimation of the model is very fast.
"""

rf = RandomForestRegressor(n_estimators = 1000,
                            n_jobs = -1,
                            max_depth = 7,
                            verbose = True,
                            )
rf.fit(x_train, y_train)
y_pred = rf.predict(x_test)

"""Interesting results - it seems that renting the whole house as opposed to just a rom has a huge, nearly 50% influence on the final price. Other factors make sence: how many people can fit in the appartment and what is the distance to the city center."""

plt.figure(figsize=(14,8))
feat_importances = pd.Series(rf.feature_importances_, index=X.columns)
feat_importances.nlargest(15).sort_values().plot(kind='barh', color='darkgrey', figsize=(10,5))
plt.xlabel('Relative Feature Importance');

"""Our model can estimate 41% of the variance in the data. The resulting pirce is typically within +- 20 euro of the original (average 40)."""

RMSE = round( mean_squared_error(y_test, y_pred, squared = False), 4)
print('RMSE: ',RMSE)

r2 = round( r2_score(y_test, y_pred), 4 )
print('R squared: ', r2)

results.append(['Random forest', r2, RMSE])

"""# XGBoost

#### Fitting sample XGBoost model
As a basis, we'll use all the default settings of XGBoost. We'll do hyperparameter tuning later.
"""

xgboost = xgb.XGBRegressor()

# train
xgboost.fit(x_train, y_train)

# predict
y_pred = xgboost.predict(x_test)

"""Now lets check importance of the variables.
Similar patters as with random forest. A notable exception is that the distance is much less important.
"""

plt.figure(figsize=(14,8))
feat_importances = pd.Series(xgboost.feature_importances_, index=X.columns)
feat_importances.nlargest(15).sort_values().plot(kind='barh', color='darkgrey', figsize=(10,5))
plt.xlabel('Relative Feature Importance');

RMSE = round( mean_squared_error(y_test, y_pred, squared = False), 4)
print('RMSE: ',RMSE)

r2 = round( r2_score(y_test, y_pred), 4 )
print('R squared: ', r2)

results.append(['XGBoost', r2, RMSE])

"""# XGBoost - hyperparameter tuning
From our expirience, one can get some nice accuracy boost from performing hyperparameter tuning on XGBoost. Number of estimators is set low to make calculations faster.
"""

xgboostCV = xgb.XGBRegressor()

# Set the parameters for the tuning
params = {
    'max_depth': [4, 5, 6, 7],
    'n_estimators': [100],
    'gamma': [0.0, 0.1, 0.3],
    'learning_rate': [0.01, 0.05, 0.15], 
    'colsample_bytree': [0.3, 0.5, 0.7, 1],
  }

# Create the tuner
cv = GridSearchCV(xgboostCV,
                  params,
                  cv = 3,
                  n_jobs=-1, 
                  verbose = 10)

# Train it
cv.fit(x_train, y_train)

# Print the best
print(cv.best_params_)

"""A total of 144 combinations tested in about 5 minutes."""

pd.DataFrame(cv.cv_results_).sort_values('rank_test_score').reset_index(drop = True)

xgboostCVBest = xgb.XGBRegressor(colsample_bytree = 0.3,
                                 gamma = 0.0,
                                 learning_rate = 0.05,
                                 max_depth = 4,
                                 n_estimators = 1000)

# train
xgboostCVBest.fit(x_train, y_train)

# predict
y_pred = xgboostCVBest.predict(x_test)

"""The results of the tuned model are within error margin of the default one."""

RMSE = round( mean_squared_error(y_test, y_pred, squared = False), 4)
print('RMSE: ',RMSE)

r2 = round( r2_score(y_test, y_pred), 4 )
print('R squared: ', r2)

results.append(['XGBoost Tuned', r2, RMSE])

"""## Neural Network
Since the size of our dataset is not that big, we'll start with a very simple neural network. 2 hidden layers of 5 nodes, with relu activation. 
We'll use an early stopping mechanism that looks at the loss on the validation data. If it doesn't improve in 5 epochs, it will stop.
"""

#Build Neural Network 
n_cols = x_train.shape[1]
hist = History()

model = Sequential()

model.add(Dense(5, activation='relu', input_dim = n_cols))
model.add(Dense(5, activation='relu'))

model.add(Dense(1))

model.compile(loss='mean_squared_error', optimizer='adam')

es_callback = EarlyStopping(monitor='val_loss', patience = 5)

hist = model.fit(x_train, y_train, epochs = 100, validation_split = .1, callbacks = [es_callback], verbose = True)  

y_pred = model.predict(x_test)

"""Significant gains in first few epochs, minimal in later rounds."""

plt.figure(figsize=(14,8))
plt.plot(hist.history['loss'], color = 'red')
plt.plot(hist.history['val_loss'], color = 'blue')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()

"""We'll add the results."""

RMSE = round( mean_squared_error(y_test, y_pred, squared = False), 4)
print('RMSE: ',RMSE)

r2 = round( r2_score(y_test, y_pred), 4 )
print('R squared: ', r2)

results.append(['Neural Network - simple', r2, RMSE])

"""# Neural Network - more complex
Maybe a more complex neural network will have better results? This one has 2 hidden layers of 200 and 100 neurons, with sigmoid and relu activation functions. We are using dropuot layers to avoid overfitting.
"""

#Build Neural Network 
n_cols = x_train.shape[1]
hist = History()

model = Sequential()

model.add(Dense(200, activation='sigmoid', input_dim = n_cols))
model.add(Dropout(0.2))

model.add(Dense(100, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(1))

model.compile(loss='mean_squared_error', optimizer='adam')

es_callback = EarlyStopping(monitor='val_loss', patience = 5)

hist = model.fit(x_train, y_train, epochs = 800, validation_split = .1, callbacks = [es_callback], verbose = True)
y_pred = model.predict(x_test)

"""Training stops very quickly, after 9 epochs."""

plt.figure(figsize=(14,8))
plt.plot(hist.history['loss'], color = 'red')
plt.plot(hist.history['val_loss'], color = 'blue')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()

"""As previously, we'll save the results."""

RMSE = round( mean_squared_error(y_test, y_pred, squared = False), 4)
print('RMSE: ',RMSE)

r2 = round( r2_score(y_test, y_pred), 4 )
print('R squared: ', r2)

results.append(['Neural Network - complex', r2, RMSE])

"""# Results
Time to review the results. This time with two metrics: R squared and root mean squared error.
"""

results2 = pd.DataFrame(results, copy = True)
results2.columns = ['Method', 'R Squared', 'RMSE']
results2.sort_values('R Squared', ascending = False)

"""XGBoost, tuned or not, performed better than other models. It was able to get R Squared up by 2 p.p.. It seems that there is not much gain from tuning the XGBoost and from increasing the complexity of the neural network.

The results were suprisingly low for us, so we search and compared them with analysis by other people:

*  **Britta Bettendorf** (author of the dataset) was able to achieve RMSE of 17.5 and R^2 of 71% using a tuned XGBoost model.
*   **Muhammed Pekta≈ü** achived RMSE of 29.14 using a complex neural network with 3 hidden layers. He did not specify the R^2
*  **Priyanshu Kumar** had RME of 17.12 and adjusted R^2 of 53.6% using a Random Forest model.

Overall, our RMSE values are quite close to best models provided by other researchers (21 vs 17), but our R^2 is much worse (41% vs 53% and 71%). One interesting observation is that in our models, the full house / one room variable is much more important than in models provided by other researchers. Other variables are simmilar.

RMSE and R^2 are connected with each other (see: https://stats.stackexchange.com/questions/142248/difference-between-r-square-and-rmse-in-linear-regression ), so they should move in a similar direction. With our current knowlage and research, we are unable to explain how is it possible to have the same dataset, simmilar RMSE and wildy different R^2 values.
"""